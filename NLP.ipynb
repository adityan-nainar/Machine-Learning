{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6wAQ5I3I+uFnNpQ4sejwG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adityan-nainar/Machine-Learning/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Natural Language Processing**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Js7rWUo5QvFB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming, Lemming and Stopwords"
      ],
      "metadata": {
        "id": "cD39NNlQ6X2q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ewoi_V-gMeX0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fcdada1-203e-4148-fed6-74f251aa6177"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"punkt\")   # for tokenization\n",
        "nltk.download(\"stopwords\")   # for stopwords\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "lemmer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = \"\"\"\n",
        "The Japan Transport Safety Board (JTSB), the South Korean Aviation and Railway Accident Investigation Board (ARAIB), and the United States National Transportation Safety Board (NTSB) all investigated the accident, with assistance from experts in South Korea and the United States. On 30 May 2016, investigators revealed that the low-pressure turbine blades on the left (number one) Pratt & Whitney PW4090 engine had \"shattered\", with fragments piercing the engine cover, with fragments subsequently found on the runway. The engine's high-pressure turbine blades and high-pressure compressor were intact and free of abnormalities, and investigators found no evidence of a bird strike.[10][11]\n",
        "\n",
        "The aircraft was repaired and returned to service with Korean Air on 3 June 2016.[12]\n",
        "\n",
        "The final JTSB investigative report, released on 26 July 2018, discussed a significant number of problems related to the failure and the response of the crew and passengers to it. These included poor maintenance standards that overlooked a crack growing in the LP turbine disc in the engine created by metal fatigue that eventually failed, the failure of the crew to locate the list of emergency procedures for use in such an emergency, beginning evacuation of the aircraft whilst the engines were still turning meaning there was a risk of passengers being blown away by the engines, and passengers ignoring instructions to leave luggage behind when using the evacuation slides risking piercing of the slides.[13] As a result of the fire, the FAA issued an Airworthiness Directive mandating inspection of engines of the type involved in the fire to evaluate the condition of the components which failed on Flight 2708.[5]:â€Š56\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "gBjRW5ngo_jJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = nltk.sent_tokenize(paragraph)\n",
        "print(sentence)\n",
        "\n",
        "stemmer.stem('going')\n",
        "lemmer.lemmatize('historical')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "VWvbqVUcfsY7",
        "outputId": "50116498-6009-4b50-bc66-60422c72be70"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\nThe Japan Transport Safety Board (JTSB), the South Korean Aviation and Railway Accident Investigation Board (ARAIB), and the United States National Transportation Safety Board (NTSB) all investigated the accident, with assistance from experts in South Korea and the United States.', 'On 30 May 2016, investigators revealed that the low-pressure turbine blades on the left (number one) Pratt & Whitney PW4090 engine had \"shattered\", with fragments piercing the engine cover, with fragments subsequently found on the runway.', \"The engine's high-pressure turbine blades and high-pressure compressor were intact and free of abnormalities, and investigators found no evidence of a bird strike.\", '[10][11]\\n\\nThe aircraft was repaired and returned to service with Korean Air on 3 June 2016.', '[12]\\n\\nThe final JTSB investigative report, released on 26 July 2018, discussed a significant number of problems related to the failure and the response of the crew and passengers to it.', 'These included poor maintenance standards that overlooked a crack growing in the LP turbine disc in the engine created by metal fatigue that eventually failed, the failure of the crew to locate the list of emergency procedures for use in such an emergency, beginning evacuation of the aircraft whilst the engines were still turning meaning there was a risk of passengers being blown away by the engines, and passengers ignoring instructions to leave luggage behind when using the evacuation slides risking piercing of the slides.', '[13] As a result of the fire, the FAA issued an Airworthiness Directive mandating inspection of engines of the type involved in the fire to evaluate the condition of the components which failed on Flight 2708.', '[5]:\\u200a56']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'historical'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BN_V_D6ZjCUs",
        "outputId": "62cd6533-93d1-45b5-e91a-b7c6f9c2c096"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re # regular expression\n",
        "\n",
        "corpus = []\n",
        "\n",
        "for i in range(len(sentence)):\n",
        "  review = re.sub('[^a-zA-z]', \" \", sentence[i])\n",
        "  review = review.lower()\n",
        "  corpus.append(review)\n",
        "\n",
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhS9ORvDhPJ_",
        "outputId": "395f834f-608e-4ad2-b5c2-f6f907ced16a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' the japan transport safety board  jtsb   the south korean aviation and railway accident investigation board  araib   and the united states national transportation safety board  ntsb  all investigated the accident  with assistance from experts in south korea and the united states ', 'on    may       investigators revealed that the low pressure turbine blades on the left  number one  pratt   whitney pw     engine had  shattered   with fragments piercing the engine cover  with fragments subsequently found on the runway ', 'the engine s high pressure turbine blades and high pressure compressor were intact and free of abnormalities  and investigators found no evidence of a bird strike ', '[  ][  ]  the aircraft was repaired and returned to service with korean air on   june      ', '[  ]  the final jtsb investigative report  released on    july       discussed a significant number of problems related to the failure and the response of the crew and passengers to it ', 'these included poor maintenance standards that overlooked a crack growing in the lp turbine disc in the engine created by metal fatigue that eventually failed  the failure of the crew to locate the list of emergency procedures for use in such an emergency  beginning evacuation of the aircraft whilst the engines were still turning meaning there was a risk of passengers being blown away by the engines  and passengers ignoring instructions to leave luggage behind when using the evacuation slides risking piercing of the slides ', '[  ] as a result of the fire  the faa issued an airworthiness directive mandating inspection of engines of the type involved in the fire to evaluate the condition of the components which failed on flight      ', '[ ]    ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in corpus:\n",
        "  words = nltk.word_tokenize(i)\n",
        "  for word in words:\n",
        "    if word not in set(stopwords.words('english')):\n",
        "      lemmer.lemmatize(word)\n",
        "      print(word)"
      ],
      "metadata": {
        "id": "gycQErBvm0Eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# cv = CountVectorizer()\n",
        "cv = CountVectorizer(binary=True, ngram_range=(2,3))   # to give binary bag of words"
      ],
      "metadata": {
        "id": "Hba8aUjcnvU3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = cv.fit_transform(corpus)"
      ],
      "metadata": {
        "id": "ZcltKbplv_Oa"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv.vocabulary_ # gives the index not the frequency"
      ],
      "metadata": {
        "id": "IQ-zLw9NwCsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[1].toarray()"
      ],
      "metadata": {
        "id": "gUI1zKfawHP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TFIDF"
      ],
      "metadata": {
        "id": "wpxoY1uNLWDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "cv = TfidfVectorizer(ngram_range=(2,3), max_features=3)\n",
        "X = cv.fit_transform(corpus)"
      ],
      "metadata": {
        "id": "uVM5lw1kwWmm"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9TGbQE8L804",
        "outputId": "fb7a871f-9f92-4b04-90a4-65773d9c1e19"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        ],\n",
              "       [0.757092  , 0.        , 0.65330828],\n",
              "       [0.        , 0.6113708 , 0.79134426],\n",
              "       [0.        , 0.36033646, 0.9328224 ],\n",
              "       [0.        , 0.        , 0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word2Vec"
      ],
      "metadata": {
        "id": "fJr-oCiIRAyu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### using BOW"
      ],
      "metadata": {
        "id": "cnXHAr8Ywelu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the Dataset\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "messages = pd.read_csv('/content/SMSSpamCollection.txt', sep='\\t',names=[\"label\", \"message\"])"
      ],
      "metadata": {
        "id": "KA4_iaK7RCYQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(messages)"
      ],
      "metadata": {
        "id": "KpPFQ-BHiUZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data cleaning and preprocessing\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vz2zmf5NjFZJ",
        "outputId": "a6fe5c23-9ff2-4cad-a128-4edae05bff07"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "ps = PorterStemmer()"
      ],
      "metadata": {
        "id": "lfo0Pa1Rog7S"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = []\n",
        "for i in range(len(messages)):\n",
        "    review = re.sub('[^a-zA-Z0-9]', ' ', messages['message'][i])\n",
        "    review = review.lower()\n",
        "    review = review.split()\n",
        "\n",
        "    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
        "    review = ' '.join(review)\n",
        "    corpus.append(review)"
      ],
      "metadata": {
        "id": "ELYnAk1Aok9I"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the Bag of Words model\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer(max_features=2500, binary=True)\n",
        "X = cv.fit_transform(corpus).toarray()"
      ],
      "metadata": {
        "id": "YB07-mIHonw-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y=pd.get_dummies(messages['label'])\n",
        "y=y.iloc[:,1].values"
      ],
      "metadata": {
        "id": "Hsfju55MpeGB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Test Split\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
      ],
      "metadata": {
        "id": "h2Vfdg38qxE_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "spam_detect_model = MultinomialNB().fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "rs7YOTmJrHfP"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prediction\n",
        "y_pred=spam_detect_model.predict(X_test)\n",
        "from sklearn.metrics import accuracy_score,classification_report"
      ],
      "metadata": {
        "id": "gDl9cA71vvjD"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score=accuracy_score(y_test,y_pred)\n",
        "print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8cLEd7hv4Wy",
        "outputId": "c84adf4a-5b7e-46ac-d8f2-49ad5dfae7a2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9865470852017937\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_pred,y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziDeypKiv75F",
        "outputId": "5d794133-db64-4067-ee58-eb5889954bf0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99       960\n",
            "           1       0.94      0.97      0.95       155\n",
            "\n",
            "    accuracy                           0.99      1115\n",
            "   macro avg       0.97      0.98      0.97      1115\n",
            "weighted avg       0.99      0.99      0.99      1115\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### using TFIFD"
      ],
      "metadata": {
        "id": "c3PZeD3bwjOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the TFIDF model\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tv = TfidfVectorizer(max_features=2500, ngram_range=(1,2))\n",
        "X = tv.fit_transform(corpus).toarray()\n",
        "# Train Test Split\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "spam_detect_model = MultinomialNB().fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "7cbi9bwNv9-O"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prediction\n",
        "y_pred=spam_detect_model.predict(X_test)\n",
        "score=accuracy_score(y_test,y_pred)\n",
        "print(score)\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_pred,y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Whsf38Brwled",
        "outputId": "46664b63-85db-4cf1-a7b5-9e9a966426bb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9811659192825112\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.98      0.99       976\n",
            "           1       0.87      1.00      0.93       139\n",
            "\n",
            "    accuracy                           0.98      1115\n",
            "   macro avg       0.93      0.99      0.96      1115\n",
            "weighted avg       0.98      0.98      0.98      1115\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using WORD2Vec"
      ],
      "metadata": {
        "id": "F0SqReqX1dDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugaBOQ921b_H",
        "outputId": "f74eaae3-faee-4660-cef4-7b81b5ddeeb1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "corpus = []\n",
        "for i in range(0, len(messages)):\n",
        "    review = re.sub('[^a-zA-Z]', ' ', messages['message'][i])\n",
        "    review = review.lower()\n",
        "    review = review.split()\n",
        "\n",
        "    review = [lemmatizer.lemmatize(word) for word in review if not word in stopwords.words('english')]\n",
        "    review = ' '.join(review)\n",
        "    corpus.append(review)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXW7Y8zW1hnF",
        "outputId": "4a4bc675-e9cd-4149-da5a-9e238add75af"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "\n",
        "from nltk import sent_tokenize\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "words=[]\n",
        "for sent in corpus:\n",
        "    sent_token=sent_tokenize(sent)\n",
        "    for sent in sent_token:\n",
        "        words.append(simple_preprocess(sent))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDf3_Lmy3-pu",
        "outputId": "f4ee189b-caff-476d-cef3-b17af09f4ba9"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "### Lets train Word2vec from scratch\n",
        "model=gensim.models.Word2Vec(words,window=5,min_count=2)\n",
        "model.wv.index_to_key"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HB_v0VVs4RRv",
        "outputId": "e0e36b6c-f61b-4b6a-db05-0c0575d971e6"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['call',\n",
              " 'get',\n",
              " 'ur',\n",
              " 'gt',\n",
              " 'lt',\n",
              " 'go',\n",
              " 'ok',\n",
              " 'day',\n",
              " 'free',\n",
              " 'know',\n",
              " 'come',\n",
              " 'like',\n",
              " 'time',\n",
              " 'good',\n",
              " 'got',\n",
              " 'love',\n",
              " 'text',\n",
              " 'want',\n",
              " 'send',\n",
              " 'need',\n",
              " 'one',\n",
              " 'txt',\n",
              " 'today',\n",
              " 'going',\n",
              " 'stop',\n",
              " 'home',\n",
              " 'lor',\n",
              " 'sorry',\n",
              " 'see',\n",
              " 'still',\n",
              " 'mobile',\n",
              " 'take',\n",
              " 'back',\n",
              " 'da',\n",
              " 'reply',\n",
              " 'dont',\n",
              " 'think',\n",
              " 'tell',\n",
              " 'week',\n",
              " 'hi',\n",
              " 'phone',\n",
              " 'new',\n",
              " 'later',\n",
              " 'please',\n",
              " 'pls',\n",
              " 'co',\n",
              " 'msg',\n",
              " 'min',\n",
              " 'make',\n",
              " 'night',\n",
              " 'dear',\n",
              " 'message',\n",
              " 'well',\n",
              " 'say',\n",
              " 'thing',\n",
              " 'much',\n",
              " 'oh',\n",
              " 'hope',\n",
              " 'claim',\n",
              " 'great',\n",
              " 'hey',\n",
              " 'give',\n",
              " 'number',\n",
              " 'happy',\n",
              " 'wat',\n",
              " 'friend',\n",
              " 'work',\n",
              " 'way',\n",
              " 'yes',\n",
              " 'www',\n",
              " 'prize',\n",
              " 'let',\n",
              " 'right',\n",
              " 'tomorrow',\n",
              " 'already',\n",
              " 'tone',\n",
              " 'ask',\n",
              " 'win',\n",
              " 'said',\n",
              " 'life',\n",
              " 'cash',\n",
              " 'amp',\n",
              " 'yeah',\n",
              " 'im',\n",
              " 'really',\n",
              " 'meet',\n",
              " 'babe',\n",
              " 'find',\n",
              " 'miss',\n",
              " 'morning',\n",
              " 'thanks',\n",
              " 'last',\n",
              " 'uk',\n",
              " 'service',\n",
              " 'year',\n",
              " 'anything',\n",
              " 'care',\n",
              " 'would',\n",
              " 'com',\n",
              " 'also',\n",
              " 'lol',\n",
              " 'nokia',\n",
              " 'feel',\n",
              " 'every',\n",
              " 'keep',\n",
              " 'sure',\n",
              " 'pick',\n",
              " 'urgent',\n",
              " 'sent',\n",
              " 'contact',\n",
              " 'something',\n",
              " 'buy',\n",
              " 'gud',\n",
              " 'cant',\n",
              " 'wait',\n",
              " 'box',\n",
              " 'place',\n",
              " 'first',\n",
              " 'even',\n",
              " 'someone',\n",
              " 'help',\n",
              " 'guy',\n",
              " 'nice',\n",
              " 'went',\n",
              " 'tonight',\n",
              " 'next',\n",
              " 'wish',\n",
              " 'around',\n",
              " 'soon',\n",
              " 'show',\n",
              " 'word',\n",
              " 'could',\n",
              " 'customer',\n",
              " 'money',\n",
              " 'sleep',\n",
              " 'late',\n",
              " 'many',\n",
              " 'per',\n",
              " 'chat',\n",
              " 'always',\n",
              " 'gonna',\n",
              " 'ya',\n",
              " 'sm',\n",
              " 'leave',\n",
              " 'wan',\n",
              " 'name',\n",
              " 'lot',\n",
              " 'dun',\n",
              " 'end',\n",
              " 'pm',\n",
              " 'st',\n",
              " 'told',\n",
              " 'special',\n",
              " 'hello',\n",
              " 'person',\n",
              " 'waiting',\n",
              " 'may',\n",
              " 'try',\n",
              " 'month',\n",
              " 'hour',\n",
              " 'fine',\n",
              " 'girl',\n",
              " 'haha',\n",
              " 'people',\n",
              " 'heart',\n",
              " 'coming',\n",
              " 'minute',\n",
              " 'best',\n",
              " 'th',\n",
              " 'getting',\n",
              " 'yet',\n",
              " 'smile',\n",
              " 'done',\n",
              " 'thk',\n",
              " 'guaranteed',\n",
              " 'ppm',\n",
              " 'god',\n",
              " 'thought',\n",
              " 'use',\n",
              " 'offer',\n",
              " 'holiday',\n",
              " 'start',\n",
              " 'class',\n",
              " 'stuff',\n",
              " 'talk',\n",
              " 'man',\n",
              " 'car',\n",
              " 'lunch',\n",
              " 'cost',\n",
              " 'line',\n",
              " 'live',\n",
              " 'mean',\n",
              " 'bit',\n",
              " 'job',\n",
              " 'finish',\n",
              " 'draw',\n",
              " 'problem',\n",
              " 'never',\n",
              " 'plan',\n",
              " 'better',\n",
              " 'meeting',\n",
              " 'hr',\n",
              " 'thats',\n",
              " 'trying',\n",
              " 'house',\n",
              " 'yup',\n",
              " 'ill',\n",
              " 'cool',\n",
              " 'rate',\n",
              " 'mind',\n",
              " 'long',\n",
              " 'dat',\n",
              " 'pobox',\n",
              " 'account',\n",
              " 'ready',\n",
              " 'weekend',\n",
              " 'chance',\n",
              " 'game',\n",
              " 'real',\n",
              " 'enjoy',\n",
              " 'half',\n",
              " 'world',\n",
              " 'wk',\n",
              " 'latest',\n",
              " 'bt',\n",
              " 'po',\n",
              " 'play',\n",
              " 'yo',\n",
              " 'guess',\n",
              " 'sir',\n",
              " 'check',\n",
              " 'room',\n",
              " 'wanna',\n",
              " 'sweet',\n",
              " 'eat',\n",
              " 'camera',\n",
              " 'voucher',\n",
              " 'nothing',\n",
              " 'pic',\n",
              " 'look',\n",
              " 'receive',\n",
              " 'luv',\n",
              " 'lar',\n",
              " 'boy',\n",
              " 'awarded',\n",
              " 'another',\n",
              " 'shit',\n",
              " 'big',\n",
              " 'liao',\n",
              " 'landline',\n",
              " 'dinner',\n",
              " 'birthday',\n",
              " 'ah',\n",
              " 'xxx',\n",
              " 'age',\n",
              " 'video',\n",
              " 'jus',\n",
              " 'quite',\n",
              " 'ever',\n",
              " 'kiss',\n",
              " 'might',\n",
              " 'watching',\n",
              " 'wont',\n",
              " 'land',\n",
              " 'question',\n",
              " 'watch',\n",
              " 'tv',\n",
              " 'early',\n",
              " 'fun',\n",
              " 'probably',\n",
              " 'orange',\n",
              " 'bed',\n",
              " 'dream',\n",
              " 'aight',\n",
              " 'hear',\n",
              " 'thanx',\n",
              " 'two',\n",
              " 'worry',\n",
              " 'baby',\n",
              " 'speak',\n",
              " 'pa',\n",
              " 'nd',\n",
              " 'point',\n",
              " 'called',\n",
              " 'nite',\n",
              " 'maybe',\n",
              " 'apply',\n",
              " 'left',\n",
              " 'bus',\n",
              " 'forgot',\n",
              " 'ringtone',\n",
              " 'actually',\n",
              " 'sat',\n",
              " 'network',\n",
              " 'princess',\n",
              " 'bad',\n",
              " 'remember',\n",
              " 'den',\n",
              " 'shall',\n",
              " 'pay',\n",
              " 'part',\n",
              " 'code',\n",
              " 'shopping',\n",
              " 'office',\n",
              " 'reach',\n",
              " 'made',\n",
              " 'dunno',\n",
              " 'hurt',\n",
              " 'easy',\n",
              " 'fuck',\n",
              " 'leh',\n",
              " 'dad',\n",
              " 'face',\n",
              " 'little',\n",
              " 'everything',\n",
              " 'anyway',\n",
              " 'wife',\n",
              " 'true',\n",
              " 'xx',\n",
              " 'put',\n",
              " 'didnt',\n",
              " 'evening',\n",
              " 'award',\n",
              " 'dis',\n",
              " 'afternoon',\n",
              " 'town',\n",
              " 'movie',\n",
              " 'school',\n",
              " 'gift',\n",
              " 'enough',\n",
              " 'mate',\n",
              " 'sound',\n",
              " 'thank',\n",
              " 'working',\n",
              " 'looking',\n",
              " 'selected',\n",
              " 'yr',\n",
              " 'mail',\n",
              " 'entry',\n",
              " 'missing',\n",
              " 'pound',\n",
              " 'collect',\n",
              " 'asked',\n",
              " 'detail',\n",
              " 'tmr',\n",
              " 'without',\n",
              " 'though',\n",
              " 'join',\n",
              " 'important',\n",
              " 'hav',\n",
              " 'wif',\n",
              " 'must',\n",
              " 'xmas',\n",
              " 'wanted',\n",
              " 'pain',\n",
              " 'sexy',\n",
              " 'came',\n",
              " 'valid',\n",
              " 'okay',\n",
              " 'since',\n",
              " 'price',\n",
              " 'answer',\n",
              " 'wot',\n",
              " 'abt',\n",
              " 'lesson',\n",
              " 'able',\n",
              " 'wake',\n",
              " 'collection',\n",
              " 'til',\n",
              " 'update',\n",
              " 'mob',\n",
              " 'run',\n",
              " 'book',\n",
              " 'missed',\n",
              " 'bring',\n",
              " 'plus',\n",
              " 'stay',\n",
              " 'plz',\n",
              " 'decimal',\n",
              " 'charge',\n",
              " 'date',\n",
              " 'away',\n",
              " 'de',\n",
              " 'juz',\n",
              " 'wen',\n",
              " 'test',\n",
              " 'change',\n",
              " 'colour',\n",
              " 'alright',\n",
              " 'hair',\n",
              " 'bored',\n",
              " 'double',\n",
              " 'attempt',\n",
              " 'music',\n",
              " 'yesterday',\n",
              " 'weekly',\n",
              " 'else',\n",
              " 'till',\n",
              " 'shop',\n",
              " 'dude',\n",
              " 'saw',\n",
              " 'havent',\n",
              " 'goin',\n",
              " 'online',\n",
              " 'drink',\n",
              " 'optout',\n",
              " 'friendship',\n",
              " 'oso',\n",
              " 'id',\n",
              " 'lei',\n",
              " 'top',\n",
              " 'net',\n",
              " 'haf',\n",
              " 'trip',\n",
              " 'credit',\n",
              " 'coz',\n",
              " 'making',\n",
              " 'food',\n",
              " 'player',\n",
              " 'either',\n",
              " 'sch',\n",
              " 'feeling',\n",
              " 'family',\n",
              " 'national',\n",
              " 'ard',\n",
              " 'hot',\n",
              " 'delivery',\n",
              " 'address',\n",
              " 'club',\n",
              " 'driving',\n",
              " 'gr',\n",
              " 'smoke',\n",
              " 'tried',\n",
              " 'http',\n",
              " 'lose',\n",
              " 'mom',\n",
              " 'full',\n",
              " 'wid',\n",
              " 'sae',\n",
              " 'bonus',\n",
              " 'head',\n",
              " 'post',\n",
              " 'second',\n",
              " 'walk',\n",
              " 'beautiful',\n",
              " 'ring',\n",
              " 'calling',\n",
              " 'busy',\n",
              " 'order',\n",
              " 'story',\n",
              " 'si',\n",
              " 'sad',\n",
              " 'believe',\n",
              " 'brother',\n",
              " 'together',\n",
              " 'tot',\n",
              " 'nt',\n",
              " 'mum',\n",
              " 'happen',\n",
              " 'close',\n",
              " 'smiling',\n",
              " 'await',\n",
              " 'hand',\n",
              " 'info',\n",
              " 'drive',\n",
              " 'old',\n",
              " 'leaving',\n",
              " 'sleeping',\n",
              " 'row',\n",
              " 'chikku',\n",
              " 'huh',\n",
              " 'set',\n",
              " 'saying',\n",
              " 'poly',\n",
              " 'eve',\n",
              " 'noe',\n",
              " 'email',\n",
              " 'private',\n",
              " 'started',\n",
              " 'finished',\n",
              " 'match',\n",
              " 'hl',\n",
              " 'drop',\n",
              " 'okie',\n",
              " 'cause',\n",
              " 'news',\n",
              " 'took',\n",
              " 'congrats',\n",
              " 'parent',\n",
              " 'mths',\n",
              " 'suite',\n",
              " 'aft',\n",
              " 'tomo',\n",
              " 'rite',\n",
              " 'pub',\n",
              " 'thinking',\n",
              " 'wil',\n",
              " 'awesome',\n",
              " 'simple',\n",
              " 'forget',\n",
              " 'unsubscribe',\n",
              " 'auction',\n",
              " 'reason',\n",
              " 'caller',\n",
              " 'content',\n",
              " 'available',\n",
              " 'neva',\n",
              " 'sister',\n",
              " 'mine',\n",
              " 'anyone',\n",
              " 'final',\n",
              " 'tho',\n",
              " 'gd',\n",
              " 'card',\n",
              " 'valentine',\n",
              " 'angry',\n",
              " 'tc',\n",
              " 'company',\n",
              " 'taking',\n",
              " 'break',\n",
              " 'statement',\n",
              " 'everyone',\n",
              " 'touch',\n",
              " 'expires',\n",
              " 'whats',\n",
              " 'open',\n",
              " 'type',\n",
              " 'search',\n",
              " 'treat',\n",
              " 'found',\n",
              " 'opt',\n",
              " 'dating',\n",
              " 'sun',\n",
              " 'whatever',\n",
              " 'knw',\n",
              " 'ticket',\n",
              " 'alone',\n",
              " 'fancy',\n",
              " 'choose',\n",
              " 'lucky',\n",
              " 'bank',\n",
              " 'carlos',\n",
              " 'gal',\n",
              " 'worth',\n",
              " 'loving',\n",
              " 'hows',\n",
              " 'bout',\n",
              " 'welcome',\n",
              " 'smth',\n",
              " 'ha',\n",
              " 'saturday',\n",
              " 'exam',\n",
              " 'uncle',\n",
              " 'happened',\n",
              " 'party',\n",
              " 'identifier',\n",
              " 'quiz',\n",
              " 'kind',\n",
              " 'nyt',\n",
              " 'hard',\n",
              " 'visit',\n",
              " 'college',\n",
              " 'wonderful',\n",
              " 'sub',\n",
              " 'frnd',\n",
              " 'fast',\n",
              " 'winner',\n",
              " 'mobileupd',\n",
              " 'ni',\n",
              " 'boytoy',\n",
              " 'ltd',\n",
              " 'decided',\n",
              " 'friday',\n",
              " 'gbp',\n",
              " 'anytime',\n",
              " 'prob',\n",
              " 'song',\n",
              " 'hit',\n",
              " 'gone',\n",
              " 'far',\n",
              " 'congratulation',\n",
              " 'secret',\n",
              " 'used',\n",
              " 'project',\n",
              " 'tel',\n",
              " 'oredi',\n",
              " 'finally',\n",
              " 'goodmorning',\n",
              " 'mu',\n",
              " 'pretty',\n",
              " 'sea',\n",
              " 'light',\n",
              " 'read',\n",
              " 'term',\n",
              " 'nope',\n",
              " 'darlin',\n",
              " 'mrng',\n",
              " 'outside',\n",
              " 'fri',\n",
              " 'camcorder',\n",
              " 'fucking',\n",
              " 'operator',\n",
              " 'crazy',\n",
              " 'wit',\n",
              " 'drug',\n",
              " 'chennai',\n",
              " 'hold',\n",
              " 'wonder',\n",
              " 'lovely',\n",
              " 'least',\n",
              " 'wrong',\n",
              " 'support',\n",
              " 'blue',\n",
              " 'savamob',\n",
              " 'earlier',\n",
              " 'snow',\n",
              " 'wkly',\n",
              " 'fone',\n",
              " 'frm',\n",
              " 'freemsg',\n",
              " 'course',\n",
              " 'whole',\n",
              " 'frnds',\n",
              " 'log',\n",
              " 'cd',\n",
              " 'le',\n",
              " 'listen',\n",
              " 'meant',\n",
              " 'sunday',\n",
              " 'hmm',\n",
              " 'hungry',\n",
              " 'jay',\n",
              " 'case',\n",
              " 'ten',\n",
              " 'unlimited',\n",
              " 'fr',\n",
              " 'wq',\n",
              " 'telling',\n",
              " 'seeing',\n",
              " 'cum',\n",
              " 'john',\n",
              " 'rock',\n",
              " 'currently',\n",
              " 'mr',\n",
              " 'father',\n",
              " 'india',\n",
              " 'understand',\n",
              " 'hmmm',\n",
              " 'as',\n",
              " 'dnt',\n",
              " 'gas',\n",
              " 'knew',\n",
              " 'hee',\n",
              " 'motorola',\n",
              " 'enter',\n",
              " 'invited',\n",
              " 'side',\n",
              " 'felt',\n",
              " 'child',\n",
              " 'store',\n",
              " 'download',\n",
              " 'gn',\n",
              " 'moment',\n",
              " 'na',\n",
              " 'film',\n",
              " 'luck',\n",
              " 'couple',\n",
              " 'mah',\n",
              " 'single',\n",
              " 'christmas',\n",
              " 'sex',\n",
              " 'stupid',\n",
              " 'etc',\n",
              " 'reading',\n",
              " 'within',\n",
              " 'un',\n",
              " 'balance',\n",
              " 'almost',\n",
              " 'tired',\n",
              " 'valued',\n",
              " 'lost',\n",
              " 'eh',\n",
              " 'yar',\n",
              " 'computer',\n",
              " 'pas',\n",
              " 'press',\n",
              " 'happiness',\n",
              " 'joy',\n",
              " 'txts',\n",
              " 'move',\n",
              " 'area',\n",
              " 'cut',\n",
              " 'bslvyl',\n",
              " 'march',\n",
              " 'paper',\n",
              " 'die',\n",
              " 'sk',\n",
              " 'load',\n",
              " 'park',\n",
              " 'ago',\n",
              " 'mayb',\n",
              " 'talking',\n",
              " 'sell',\n",
              " 'gym',\n",
              " 'wow',\n",
              " 'ac',\n",
              " 'difficult',\n",
              " 'surprise',\n",
              " 'askd',\n",
              " 'ugh',\n",
              " 'complimentary',\n",
              " 'shower',\n",
              " 'gotta',\n",
              " 'photo',\n",
              " 'ipod',\n",
              " 'direct',\n",
              " 'comp',\n",
              " 'red',\n",
              " 'return',\n",
              " 'via',\n",
              " 'darren',\n",
              " 'laptop',\n",
              " 'reveal',\n",
              " 'max',\n",
              " 'an',\n",
              " 'figure',\n",
              " 'bcoz',\n",
              " 'ish',\n",
              " 'extra',\n",
              " 'hospital',\n",
              " 'promise',\n",
              " 'sending',\n",
              " 'heard',\n",
              " 'grin',\n",
              " 'bill',\n",
              " 'information',\n",
              " 'swing',\n",
              " 'xy',\n",
              " 'confirm',\n",
              " 'rental',\n",
              " 'picking',\n",
              " 'kid',\n",
              " 'charged',\n",
              " 'doin',\n",
              " 'ge',\n",
              " 'supposed',\n",
              " 'redeemed',\n",
              " 'seen',\n",
              " 'semester',\n",
              " 'correct',\n",
              " 'eye',\n",
              " 'met',\n",
              " 'lady',\n",
              " 'sort',\n",
              " 'fact',\n",
              " 'discount',\n",
              " 'hg',\n",
              " 'eg',\n",
              " 'comin',\n",
              " 'study',\n",
              " 'checking',\n",
              " 'fantasy',\n",
              " 'ex',\n",
              " 'std',\n",
              " 'request',\n",
              " 'road',\n",
              " 'clean',\n",
              " 'hmv',\n",
              " 'asap',\n",
              " 'leaf',\n",
              " 'train',\n",
              " 'txting',\n",
              " 'wana',\n",
              " 'whenever',\n",
              " 'noon',\n",
              " 'reward',\n",
              " 'somebody',\n",
              " 'lover',\n",
              " 'door',\n",
              " 'police',\n",
              " 'slowly',\n",
              " 'rest',\n",
              " 'loved',\n",
              " 'wap',\n",
              " 'link',\n",
              " 'pete',\n",
              " 'laugh',\n",
              " 'lovable',\n",
              " 'joke',\n",
              " 'blood',\n",
              " 'small',\n",
              " 'truth',\n",
              " 'weed',\n",
              " 'slow',\n",
              " 'usf',\n",
              " 'entered',\n",
              " 'kate',\n",
              " 'yep',\n",
              " 'rent',\n",
              " 'crave',\n",
              " 'idea',\n",
              " 'loan',\n",
              " 'safe',\n",
              " 'rply',\n",
              " 'abiola',\n",
              " 'nobody',\n",
              " 'monday',\n",
              " 'page',\n",
              " 'remove',\n",
              " 'bath',\n",
              " 'cheer',\n",
              " 'muz',\n",
              " 'save',\n",
              " 'asking',\n",
              " 'orchard',\n",
              " 'dogging',\n",
              " 'member',\n",
              " 'wine',\n",
              " 'write',\n",
              " 'sale',\n",
              " 'med',\n",
              " 'copy',\n",
              " 'la',\n",
              " 'spend',\n",
              " 'callertune',\n",
              " 'normal',\n",
              " 'convey',\n",
              " 'reached',\n",
              " 'worried',\n",
              " 'merry',\n",
              " 'ldn',\n",
              " 'voice',\n",
              " 'mistake',\n",
              " 'bb',\n",
              " 'cover',\n",
              " 'cheap',\n",
              " 'ringtones',\n",
              " 'immediately',\n",
              " 'hoping',\n",
              " 'warm',\n",
              " 'getzed',\n",
              " 'deep',\n",
              " 'il',\n",
              " 'poor',\n",
              " 'gap',\n",
              " 'gave',\n",
              " 'em',\n",
              " 'different',\n",
              " 'usual',\n",
              " 'men',\n",
              " 'otherwise',\n",
              " 'ntt',\n",
              " 'cr',\n",
              " 'doctor',\n",
              " 'indian',\n",
              " 'oops',\n",
              " 'glad',\n",
              " 'ho',\n",
              " 'tonite',\n",
              " 'energy',\n",
              " 'pray',\n",
              " 'sony',\n",
              " 'somewhere',\n",
              " 'del',\n",
              " 'booked',\n",
              " 'wc',\n",
              " 'mm',\n",
              " 'fantastic',\n",
              " 'custcare',\n",
              " 'summer',\n",
              " 'opinion',\n",
              " 'forever',\n",
              " 'teach',\n",
              " 'rakhesh',\n",
              " 'deal',\n",
              " 'representative',\n",
              " 'hw',\n",
              " 'street',\n",
              " 'across',\n",
              " 'catch',\n",
              " 'king',\n",
              " 'bag',\n",
              " 'empty',\n",
              " 'woke',\n",
              " 'near',\n",
              " 'england',\n",
              " 'situation',\n",
              " 'admirer',\n",
              " 'short',\n",
              " 'cup',\n",
              " 'kick',\n",
              " 'turn',\n",
              " 'water',\n",
              " 'bathe',\n",
              " 'gettin',\n",
              " 'rd',\n",
              " 'wishing',\n",
              " 'nah',\n",
              " 'bluetooth',\n",
              " 'self',\n",
              " 'flag',\n",
              " 'ice',\n",
              " 'sunshine',\n",
              " 'style',\n",
              " 'rose',\n",
              " 'unless',\n",
              " 'result',\n",
              " 'reference',\n",
              " 'excellent',\n",
              " 'kinda',\n",
              " 'specially',\n",
              " 'tear',\n",
              " 'reaching',\n",
              " 'digital',\n",
              " 'sick',\n",
              " 'none',\n",
              " 'decide',\n",
              " 'seriously',\n",
              " 'sport',\n",
              " 'moral',\n",
              " 'flight',\n",
              " 'rcvd',\n",
              " 'bos',\n",
              " 'access',\n",
              " 'mon',\n",
              " 'mo',\n",
              " 'round',\n",
              " 'urself',\n",
              " 'al',\n",
              " 'logo',\n",
              " 'cake',\n",
              " 'iam',\n",
              " 'gay',\n",
              " 'ending',\n",
              " 'meh',\n",
              " 'hotel',\n",
              " 'brings',\n",
              " 'sofa',\n",
              " 'buying',\n",
              " 'accept',\n",
              " 'coffee',\n",
              " 'hiya',\n",
              " 'password',\n",
              " 'silent',\n",
              " 'mode',\n",
              " 'tht',\n",
              " 'wondering',\n",
              " 'others',\n",
              " 'rain',\n",
              " 'add',\n",
              " 'user',\n",
              " 'especially',\n",
              " 'disturb',\n",
              " 'trust',\n",
              " 'thinkin',\n",
              " 'possible',\n",
              " 'fall',\n",
              " 'bx',\n",
              " 'using',\n",
              " 'bold',\n",
              " 'ip',\n",
              " 'dead',\n",
              " 'ive',\n",
              " 'colleague',\n",
              " 'excuse',\n",
              " 'lazy',\n",
              " 'norm',\n",
              " 'ldew',\n",
              " 'bid',\n",
              " 'starting',\n",
              " 'lect',\n",
              " 'comuk',\n",
              " 'goodnight',\n",
              " 'charity',\n",
              " 'pc',\n",
              " 'studying',\n",
              " 'mrt',\n",
              " 'forwarded',\n",
              " 'seems',\n",
              " 'tampa',\n",
              " 'stand',\n",
              " 'completely',\n",
              " 'sitting',\n",
              " 'staying',\n",
              " 'doesnt',\n",
              " 'slave',\n",
              " 'finger',\n",
              " 'fat',\n",
              " 'tuesday',\n",
              " 'apartment',\n",
              " 'cute',\n",
              " 'file',\n",
              " 'wednesday',\n",
              " 'fill',\n",
              " 'list',\n",
              " 'issue',\n",
              " 'slept',\n",
              " 'miracle',\n",
              " 'omg',\n",
              " 'er',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.corpus_count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JsJL7xs4tQY",
        "outputId": "8f02c4d7-6e2e-4e38-970f-9708270fac48"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5564"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.epochs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zuz7PGJV5zP5",
        "outputId": "9fa1b061-253d-452a-bb05-80ebbf1eef61"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.similar_by_word('king')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JB2aPuT51WM",
        "outputId": "425c5762-709b-41d9-b6ec-a4384283c0ba"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('ready', 0.9941062927246094),\n",
              " ('night', 0.9940164089202881),\n",
              " ('thats', 0.9938697218894958),\n",
              " ('little', 0.9938569068908691),\n",
              " ('dear', 0.9938194155693054),\n",
              " ('well', 0.993813693523407),\n",
              " ('keep', 0.993794858455658),\n",
              " ('sure', 0.9937848448753357),\n",
              " ('take', 0.9937664270401001),\n",
              " ('use', 0.993759036064148)]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "pzP-8dgZ8bHy"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def avg_word2vec(doc):\n",
        "    # remove out-of-vocabulary words\n",
        "    #sent = [word for word in doc if word in model.wv.index_to_key]\n",
        "    #print(sent)\n",
        "\n",
        "    return np.mean([model.wv[word] for word in doc if word in model.wv.index_to_key],axis=0)\n",
        "                #or [np.zeros(len(model.wv.index_to_key))], axis=0)"
      ],
      "metadata": {
        "id": "YHEpqsUU6HRz"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm\n",
        "\n",
        "from tqdm import tqdm\n",
        "words[73]\n",
        "type(model.wv.index_to_key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8pClw0E64mw",
        "outputId": "ab4e6ea0-9fa3-48db-ebcb-39b21a96229e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#apply for the entire sentences\n",
        "X=[]\n",
        "for i in tqdm(range(len(words))):\n",
        "    print(\"Hello\",i)\n",
        "    X.append(avg_word2vec(words[i]))"
      ],
      "metadata": {
        "id": "0AzPH_Ih69eU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PrUR5Zgv8Q5t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}